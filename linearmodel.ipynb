{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "master_df = pd.read_csv('master.csv')\n",
    "master_df2 = pd.read_csv('master2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['full_moon_indicator', 'is_holiday'], dtype='object')\n",
      "Index(['Unnamed: 0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "columns_master_df = master_df.columns\n",
    "columns_master_df2 = master_df2.columns\n",
    "\n",
    "# Find columns in master_df2 that are not in master_df\n",
    "extra_columns = columns_master_df2.difference(columns_master_df)\n",
    "print(extra_columns)\n",
    "\n",
    "# Get column names of master_df and master_df2\n",
    "columns_master_df = master_df.columns\n",
    "columns_master_df2 = master_df2.columns\n",
    "\n",
    "# Find columns in master_df that are not in master_df2\n",
    "extra_columns = columns_master_df.difference(columns_master_df2)\n",
    "print(extra_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day of Week          0\n",
       "feelslikemax         0\n",
       "feelslikemin         0\n",
       "precip               0\n",
       "snowdepth            0\n",
       "windgust           284\n",
       "windspeed            0\n",
       "solarradiation       0\n",
       "solarenergy          0\n",
       "uvindex              0\n",
       "moonphase            0\n",
       "Holiday           4958\n",
       "cloudcover           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "master_df = pd.read_csv('master2.csv')\n",
    "\n",
    "\n",
    "# Assuming master_df contains your data and 'Date' is a datetime column\n",
    "data = master_df.copy()\n",
    "####!!!!\n",
    "# duplicate_dates = data[data.duplicated(subset=['Date'], keep=False)]\n",
    "\n",
    "# if duplicate_dates.empty:\n",
    "#     print(\"No repeat dates found.\")\n",
    "# else:\n",
    "#     print(\"Repeat dates found:\")\n",
    "#     print(duplicate_dates)\n",
    "\n",
    "# data\n",
    "\n",
    "\n",
    "# # Create a DataFrame with all dates between 01/01/2010 and 2/24/2024\n",
    "# start_date = pd.to_datetime('01/01/2010')\n",
    "# end_date = pd.to_datetime('2/24/2024')\n",
    "# all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "# all_dates_df = pd.DataFrame({'Date': all_dates})\n",
    "\n",
    "# # Assuming you have another DataFrame df with the \"Date\" column\n",
    "# # Make sure the \"Date\" column in df is in datetime format\n",
    "# # df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# # Find the missing dates\n",
    "# missing_dates = all_dates_df[~all_dates_df['Date'].isin(data['Date'])]\n",
    "\n",
    "# print(\"Missing Dates:\")\n",
    "# print(missing_dates)\n",
    "columns_to_check =['Day of Week', 'feelslikemax', 'feelslikemin', 'precip', 'snowdepth', \n",
    "          'windgust', 'windspeed', 'solarradiation', 'solarenergy', \n",
    "          'uvindex', 'moonphase', 'Holiday', 'cloudcover']\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = data[columns_to_check].isna().sum()\n",
    "data['Holiday'].fillna('non-holiday', inplace=True)\n",
    "\n",
    "# data.dropna(subset=columns_to_check, inplace=True)\n",
    "data\n",
    "missing_values_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Drop unnecessary columns and missing values if needed\n",
    "# data.dropna(inplace=True)  # Uncomment this line if you have missing values and want to drop them\n",
    "data['moonphase2'] = abs(data['moonphase'] - 0.5)\n",
    "data.to_csv('data.csv',index=False)  # Set index=False to exclude the index column from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = data[['Day of Week', 'feelslike', 'precip', 'snowdepth', 'windspeed', \n",
    "          'uvindex', 'moonphase2', 'Holiday', 'cloudcover']]\n",
    "y = data['count']\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=500,max_depth=20, min_samples_leaf=2, min_samples_split=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted counts\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Plot feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, feature_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50,100, 150],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2],\n",
    "#     # 'max_features': ['auto', 'sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # Create the Random Forest Regressor model\n",
    "# rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Perform Grid Search with cross-validation\n",
    "# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Score (MSE):\", -best_score)\n",
    "\n",
    "# # Alternatively, you can use Randomized Search\n",
    "# # Define the parameter distributions for Randomized Search\n",
    "# param_dist = {\n",
    "#     'n_estimators': [50,100,150],\n",
    "#     'max_depth': [None] + list(range(5, 31, 5)),\n",
    "#     'min_samples_split': [10,20],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     # 'max_features': ['auto', 'sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # Perform Randomized Search with cross-validation\n",
    "# random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, n_iter=100, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = random_search.best_params_\n",
    "# best_score = random_search.best_score_\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Score (MSE):\", -best_score)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_test, color='blue', alpha=0.5, label='Actual Counts')\n",
    "sns.histplot(y_pred, color='orange', alpha=0.5, label='Predicted Counts')\n",
    "plt.title('Distribution of Actual vs. Predicted Counts')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "residuals = y_pred - y_test\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = pd.get_dummies(data, columns=['Holiday', 'Day of Week'])\n",
    "encoded_data\n",
    "encoded_data.to_csv('data.csv', index=False)  # Specify index=False if you don't want to write row indices to the CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'OneHotEncoder' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Transform the input data using the preprocessor\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get the names of the transformed features after one-hot encoding\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Get the names of the transformed features after one-hot encoding\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m transformed_feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreprocessor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_transformers_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Combine the names of numerical and categorical features\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'OneHotEncoder' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Select relevant columns\n",
    "explanatory_vars = ['Day of Week', 'feelslike', 'precip', 'snowdepth', 'windspeed', 'uvindex', 'moonphase2', 'Holiday', 'cloudcover']\n",
    "response_var = ['count']\n",
    "\n",
    "# Split features and target variable\n",
    "X = data[explanatory_vars]\n",
    "y = data[response_var]\n",
    "\n",
    "# One-hot encode categorical variables and normalize all variables\n",
    "categorical_cols = ['Day of Week', 'Holiday']\n",
    "numerical_cols = ['feelslike', 'precip', 'snowdepth', 'windspeed', 'uvindex', 'moonphase2', 'cloudcover']\n",
    "\n",
    "# Create a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on the testing set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
